{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLX Ultralytics Object Detection Training (Colab)\n\nThis notebook mirrors the MLX object detection workflow in Google Colab. It executes the following steps:\n1. Clone the MLX repository.\n2. Install the custom Ultralytics fork and MLX dependencies.\n3. Capture the required API credentials.\n4. Download and extract the DroneFace YOLO12 dataset from Roboflow.\n5. Write the provided YOLO12 model configuration.\n6. Launch training through the MLX CLI.\n\n> \u26a0\ufe0f Switch your Colab runtime to **GPU** for reasonable training speed (`Runtime \u2192 Change runtime type \u2192 GPU`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0 \u00b7 Clone the MLX repository\nThis pulls the public repository and switches the working directory so subsequent installs run in-place.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = 'https://github.com/ralampay/mlx'\n",
        "cwd = Path.cwd()\n",
        "if cwd.name != 'mlx':\n",
        "    repo_dir = cwd / 'mlx'\n",
        "    if not repo_dir.exists():\n",
        "        subprocess.run(['git', 'clone', REPO_URL], check=True)\n",
        "    os.chdir(repo_dir)\n",
        "print('Working directory:', Path.cwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 \u00b7 Install the ralampay Ultralytics fork\nInstalls the YOLO implementation that MLX expects for object detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '--upgrade', 'pip'], check=True)\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', 'git+https://github.com/ralampay/ultralytics'], check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 \u00b7 Install MLX and supporting dependencies\nThe editable install exposes the `mlx` CLI. The Roboflow SDK handles dataset downloads.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '-e', '.'], check=True)\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', 'roboflow'], check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 \u00b7 Supply API credentials\nRoboflow requires an API key to download datasets. The key is kept only in the current Colab session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if not os.environ.get('ROBOFLOW_API_KEY'):\n",
        "    os.environ['ROBOFLOW_API_KEY'] = getpass.getpass('Enter your Roboflow API Key: ')\n",
        "\n",
        "if not os.environ.get('OPENAI_API_KEY'):\n",
        "    optional_key = getpass.getpass('Optional: Enter your OpenAI API Key (leave blank to skip): ')\n",
        "    if optional_key:\n",
        "        os.environ['OPENAI_API_KEY'] = optional_key\n",
        "\n",
        "print('Roboflow key loaded:', bool(os.environ.get('ROBOFLOW_API_KEY')))\n",
        "print('OpenAI key detected:' if os.environ.get('OPENAI_API_KEY') else 'OpenAI key not set (only needed for chat module).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 \u00b7 Download and extract the DroneFace YOLO12 dataset\nThe Roboflow SDK pulls version 8 of the dataset and expands it under `datasets/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from roboflow import Roboflow\n",
        "import os\n",
        "\n",
        "api_key = os.environ.get('ROBOFLOW_API_KEY')\n",
        "if not api_key:\n",
        "    raise RuntimeError('ROBOFLOW_API_KEY is not set. Please rerun the previous cell.')\n",
        "\n",
        "rf = Roboflow(api_key=api_key)\n",
        "project = rf.workspace('face-detection-and-recognition-dataset').project('droneface')\n",
        "dataset = project.version(8).download('yolo12', location='datasets/droneface-yolo12')\n",
        "dataset_dir = Path(dataset.location)\n",
        "print('Dataset ready at:', dataset_dir)\n",
        "print('Sample contents:', [p.name for p in dataset_dir.iterdir()][:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 \u00b7 Write the `cad_yolo12.yaml` model configuration\nStores the provided YAML under `ultralytics/cfg/models/ext/cad_yolo12.yaml` so the MLX CLI can reference it directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "model_yaml_path = Path('ultralytics/cfg/models/ext/cad_yolo12.yaml')\n",
        "model_yaml_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "yaml_content = \"# Ultralytics \\ud83d\\ude80 AGPL-3.0 License - https://ultralytics.com/license\\n\\n# YOLO12 object detection model with P3/8 - P5/32 outputs\\n# Model docs: https://docs.ultralytics.com/models/yolo12\\n# Task docs: https://docs.ultralytics.com/tasks/detect\\n\\n# Parameters\\nnc: 1 # number of classes\\nscales: # model compound scaling constants, i.e. 'model=yolo12n.yaml' will call yolo12.yaml with scale 'n'\\n  # [depth, width, max_channels]\\n  n: [0.50, 0.25, 1024] # summary: 272 layers, 2,602,288 parameters, 2,602,272 gradients, 6.7 GFLOPs\\n  s: [0.50, 0.50, 1024] # summary: 272 layers, 9,284,096 parameters, 9,284,080 gradients, 21.7 GFLOPs\\n  m: [0.50, 1.00, 512] # summary: 292 layers, 20,199,168 parameters, 20,199,152 gradients, 68.1 GFLOPs\\n  l: [1.00, 1.00, 512] # summary: 488 layers, 26,450,784 parameters, 26,450,768 gradients, 89.7 GFLOPs\\n  x: [1.00, 1.50, 512] # summary: 488 layers, 59,210,784 parameters, 59,210,768 gradients, 200.3 GFLOPs\\n\\n# YOLO12n backbone\\nbackbone:\\n  # [from, repeats, module, args]\\n  - [-1, 1, ConvAttnDeform, [64, 3, 2]] # 0-P1/2\\n  - [-1, 1, ConvAttnDeform, [128, 3, 2]] # 1-P2/4\\n  - [-1, 2, C3k2, [256, False, 0.25]]\\n  - [-1, 1, ConvAttnDeform, [256, 3, 2]] # 3-P3/8\\n  - [-1, 2, C3k2, [512, False, 0.25]]\\n  - [-1, 1, ConvAttnDeform, [512, 3, 2]] # 5-P4/16\\n  - [-1, 4, A2C2f, [512, True, 4]]\\n  - [-1, 1, ConvAttnDeform, [1024, 3, 2]] # 7-P5/32\\n  - [-1, 4, A2C2f, [1024, True, 1]] # 8\\n\\n# YOLO12n head\\nhead:\\n  - [-1, 1, nn.Upsample, [None, 2, \\\"nearest\\\"]]\\n  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\\n  - [-1, 2, A2C2f, [512, False, -1]] # 11\\n\\n  - [-1, 1, nn.Upsample, [None, 2, \\\"nearest\\\"]]\\n  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\\n  - [-1, 2, A2C2f, [256, False, -1]] # 14\\n\\n  - [-1, 1, Conv, [256, 3, 2]]\\n  - [[-1, 11], 1, Concat, [1]] # cat head P4\\n  - [-1, 2, A2C2f, [512, False, -1]] # 17\\n\\n  - [-1, 1, Conv, [512, 3, 2]]\\n  - [[-1, 8], 1, Concat, [1]] # cat head P5\\n  - [-1, 2, C3k2, [1024, True]] # 20 (P5/32-large)\\n\\n  - [[14, 17, 20], 1, Detect, [nc]] # Detect(P3, P4, P5)\\n\"\n",
        "model_yaml_path.write_text(yaml_content)\n",
        "print('Model config saved to:', model_yaml_path.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 \u00b7 Launch MLX object detection training\nAdjust the hyperparameters as needed. The command streams Rich/typer output directly in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    dataset_dir\n",
        "except NameError as exc:\n",
        "    raise RuntimeError('Dataset directory not found. Please run the previous steps first.') from exc\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "except ImportError:\n",
        "    device = 'cuda:0'\n",
        "\n",
        "command = [\n",
        "    'mlx',\n",
        "    '--module', 'obj-detect',\n",
        "    '--platform', 'ultralytics',\n",
        "    '--action', 'train',\n",
        "    '--dataset-path', str(dataset_dir),\n",
        "    '--model-path', 'ultralytics/cfg/models/ext/cad_yolo12.yaml',\n",
        "    '--epochs', '100',\n",
        "    '--batch-size', '16',\n",
        "    '--device', device,\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(command))\n",
        "subprocess.run(command, check=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}